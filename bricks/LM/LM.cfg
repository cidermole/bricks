# Language model estimation
# =========================

# LanguageModelEstimator
# ----------------------
# KenLM language model estimator
#
LanguageModelEstimator: {
  input:  { corpus }
  # languageModel: binarized KenLM
  output: { languageModel }

  # from Experiment: paths for bin/lmplz used in script
  MOSES: $_.MOSES

  # n-gram order (note: 'order' is config reserved keyword)
  ngramOrder: 5

  # prune singleton trigrams and above
  prune: "0 0 1"

  template: """
    {{ brick.MOSES }}/bin/lmplz --text input/corpus --order {{ brick.ngramOrder }} \\
      --arpa output/languageModel.arpa --prune {{ brick.prune }} -T $(pwd)/output -S 20%
    {{ brick.MOSES }}/bin/build_binary output/languageModel.arpa output/languageModel
    rm -f output/languageModel.arpa
  """
}

# File
# ----
# Use language model specified as input file. Wrapper to provide
# ngramOrder for moses.ini
#
File: {
  input:  { languageModel }
  # languageModel: binarized, ideally.
  output: { languageModel }

  # n-gram order (note: 'order' is config reserved keyword)
  ngramOrder: 5

  # can we feed through directly via output ref? I guess that will confuse bricks.py
  template: """
    ln -sf ../input/languageModel output/languageModel
  """
}
