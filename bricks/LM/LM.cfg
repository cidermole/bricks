# Language model estimation
# =========================

# LanguageModelEstimator
# ----------------------
# KenLM language model estimator
#
LanguageModelEstimator: {
  input:  { corpus }
  # languageModel: binarized KenLM
  output: { languageModel }

  # from Experiment: paths for bin/lmplz used in script
  MOSES: $_.MOSES

  # n-gram order (note: 'order' is config reserved keyword)
  ngramOrder: 5

  # prune singleton trigrams and above
  prune: "0 0 1"

  # additional options (e.g. --discount_fallback for small corpora)
  extraOptions: ""

  template: """
    {{ brick.MOSES }}/bin/lmplz --text input/corpus --order {{ brick.ngramOrder }} \\
      --arpa output/languageModel.arpa --prune {{ brick.prune }} -T $(pwd)/output -S 20% \\
      {{ brick.extraOptions }}
    {{ brick.MOSES }}/bin/build_binary output/languageModel.arpa output/languageModel
    rm -f output/languageModel.arpa
  """
}

# ScoreLines
# ----------
# Scores input sentences using the provided language model.
# Outputs one log_e(p) per line, each input line being a sentence.
#
ScoreLines: {
  input:  { text, languageModel }
  output: { score }

  # from Experiment: paths for bin/query used in script
  MOSES: $_.MOSES

  template: """
    # score text, and convert each sentence score from log_10 into log_e
    {{ brick.MOSES }}/bin/query input/languageModel < input/text 2>/dev/null |
      awk '/<\/s>/ {print $(NF-2) * log(10) / (( NF - 4 ) / 3) }' > output/score
  """
}

# File
# ----
# Use language model specified as input file. Wrapper to provide
# ngramOrder for moses.ini
#
File: {
  input:  { languageModel }
  # languageModel: binarized, ideally.
  output: { languageModel }

  # n-gram order (note: 'order' is config reserved keyword)
  ngramOrder: 5

  # can we feed through directly via output ref? I guess that will confuse bricks.py
  template: """
    ln -sf ../input/languageModel output/languageModel
  """
}
