# Corpus text file manipulation
# =============================
# Cutting, shuffling and concatenating text.


# Limiter
# =======
# Outputs a certain amount of lines (sentences).
# input/output interface common to Head and Shuffle.
#
# NOTE: This is only an INTERFACE - it doesn't do anything. Use Head or Shuffle.
#
Limiter: {
  input:  { textFile }
  output: { head }

  # override to get leading nlines lines
  nlines: ALL
}

Head: {
  extends: $Limiter

  template: """
    {% if brick.nlines is number %}
      head -n {{ brick.nlines }} input/textFile > output/head
    {% else %}
      # use all lines
      cp input/textFile output/head
    {% endif %}
  """
}

Shuffle: {
  extends: $Limiter

  # deterministic random seed
  seed: 42

  template: """
    {% block Shebang %}#!/bin/bash{% endblock %}

    # deterministic random source for shuf
    # see https://www.gnu.org/software/coreutils/manual/html_node/Random-sources.html#Random-sources
    function get_seeded_random() {
      seed="$1"
      openssl enc -aes-256-ctr -pass pass:"$seed" -nosalt </dev/zero 2>/dev/null
    }

    {% if brick.nlines is number %}
      shuf -n {{ brick.nlines }} --random-source=<(get_seeded_random {{ brick.seed }}) < input/textFile > output/head
    {% else %}
      shuf --random-source=<(get_seeded_random {{ brick.seed }}) < input/textFile > output/head
    {% endif %}
  """
}

# Shuffle several corpora in the same order.
# can't use?? why? output wiring complexity.
#
ShuffleCorpora: {
  input: { textFiles: [] }
  #output: { shuffledFiles: [ $parts[$i].output.head | i: [0..$input.textFiles.length-1] ] }
  output: { shuffledFiles: [ $parts[$i].output.head ] }

  # deterministic random seed
  seed: 42

  i: [0..$input.textFiles.length-1]

  parts: [{
    extends: $Shuffle
    input: { textFile: $_._.input.textFiles[$i] }
    #output: { head }
    seed: $_.seed
  }]
}

Part: {
  extends: $Limiter

  nlinesSkip

  template: """
    {% if brick.nlines is number %}
      head -n {{ brick.nlines }} input/textFile > output/head
    {% else %}
      # use all lines
      cp input/textFile output/head
    {% endif %}

    {% if brick.nlines is number %}
      awk "NR > {{ brick.nlinesSkip }} && NR <= {{ brick.nlines + brick.nlinesSkip }}" input/textFile > output/head
    {% else %}
      awk "NR > {{ brick.nlinesSkip }}" input/textFile > output/head
    {% endif %}
  """
}

# BitextHead
# ----------
# Outputs the leading nlines of a parallel training corpus.
#
# To use all available lines, nlines may be defined without a value
# (True) or as a self-explanatory string ALL.
#
BitextHead: {
  input:  { src, trg }
  output: {
    src: $parts.HeadSrc0.output.head
    trg: $parts.HeadTrg0.output.head
  }

  nlines: ALL

  parts: {
    HeadSrc0: {
      extends: $Head
      input: { textFile: $_._.input.src }
      nlines: $_.nlines
    }
    HeadTrg0: {
      extends: $Head
      input: { textFile: $_._.input.trg }
      nlines: $_.nlines
    }
  }
}



# BitextPart
# ----------
# Outputs nlines of a parallel training corpus, starting after nlinesSkip.
#
# To use all available lines, nlines may be defined without a value
# (True) or as a self-explanatory string ALL.
#
# unused, untested.
BitextPart: {
  input:  { src, trg }
  output: {
    src: $parts.HeadSrc0.output.head
    trg: $parts.HeadTrg0.output.head
  }

  nlines: ALL
  nlinesSkip: 0

  parts: {
    HeadSrc0: {
      extends: $Head
      input: { textFile: $_._.input.src }
      nlines: $_.nlines
      nlinesSkip: $_.nlinesSkip
    }
    HeadTrg0: {
      extends: $Head
      input: { textFile: $_._.input.trg }
      nlines: $_.nlines
      nlinesSkip: $_.nlinesSkip
    }
  }
}


# Concat
# ------
# Concatenates several text corpora into one file.
#
Concat: {
  input:  { texts: [] }
  output: { concat }

  template: """
    cat{% for text in brick.input.texts %} input/texts/{{ loop.index0 }}{% endfor %} > output/concat
  """
}

# ConcatExcept
# ------------
# Concatenates several text corpora into one file, except for the given
# index which is left out.
#
ConcatExcept: {
  input:  { texts: [] }
  output: { concat }

  # index of exception
  exceptIndex

  template: """
    cat{% for text in brick.input.texts %}{% if loop.index0 != brick.exceptIndex %} input/texts/{{ loop.index0 }}{% endif %}{% endfor %} > output/concat
  """
}

# BitextConcat
# ------------
# Concatenates several parallel text corpora into one file.
#
BitextConcat: {
  input:  { sources: [], targets: [] }
  output: {
    src: $parts.ConcatSrc0.output.concat
    trg: $parts.ConcatTrg0.output.concat
  }

  parts: {
    ConcatSrc0: {
      extends: $Concat
      input:  { texts: $_._.input.sources }
    }
    ConcatTrg0: {
      extends: $Concat
      input:  { texts: $_._.input.targets }
    }
  }
}

# LineCounts
# ----------
# Outputs the length of each corpus on a separate line.
#
# unused.
LineCounts: {
  input:  { texts: [] }
  output: { countFile }

  template: """
    wc -l{% for text in brick.input.texts %} {{ text }}{% endfor %} | awk '{print $1}' | head -n-1 > output/countFile
  """
}

# Split
# -----
# Splits a single textfile into multiple lines. Uses given list of "sources" to determine line count for parts.
#
Split: {
  input: { text, sources: [] }
  output: { texts: [ True | i: [0..$input.sources.length-1] ] }

  # this is not terribly efficient, since it reads from the start for every file part.
  # can be rewritten e.g. in Python if necessary.
  template: """
    mkdir -p output/texts
    iline=1
    {% for source in brick.input.sources %}
      l=$(cat input/sources/{{ loop.index0 }} | wc -l)
      tline=$(expr $iline + $l)
      awk "NR >= $iline && NR < $tline" input/text > output/texts/{{ loop.index0 }}
      iline=$tline
    {% endfor %}
  """
}

# Does nothing. Unused. A nice template to adapt.
#
PassThrough: {
  input: { file }
  output: { file }
  template: """
    # note: ../ is important to change from output/ to brick directory
    ln -sf ../input/file output/file
  """
}

# unused, untested.
# see Part.
Subset: {
  input: { text }
  output: { text }

  # one-based line indexing, inclusive range [begin, end], e.g. [1,2] returns the first two lines.
  # 'end' may use END to go till the end of the file.

  template: """
    {% if brick.end is number %}
      awk "NR >= {{ brick.begin }} && NR <= {{ brick.end }}" input/text > output/text
    {% else %}
      awk "NR >= {{ brick.begin }}" input/text > output/text
    {% endif %}
  """
}

# SplitLines
# ----------
# Splits a single textfile into multiple lines. Uses given list of line counts for parts.
#
# unused, untested.
SplitLines: {
  input: { text }
  output: { texts: [ True | i: [0..$nlines.length-1] ] }

  # last entry may contain END
  nlines: []

  # this is not terribly efficient, since it reads from the start for every file part.
  # can be rewritten e.g. in Python if necessary.
  template: """
    mkdir -p output/texts

    iline=1
    {# cautionary: avoid evaluating output values. (unnecesary?) #}
    {% for txt in range(brick.output.texts|length) %}
      {% if brick.nlines[loop.index0] is number %}
        l={{ brick.nlines[loop.index0] }}
      {% else %}
        l=$(expr $(cat input/text | wc -l) - $iline + 1)
      {% endif %}
      tline=$(expr $iline + $l)
      # rm -f output/texts/{{ loop.index0 }}  # remove back-symlink and replace it with a file (part of workaround)
      awk "NR >= $iline && NR < $tline" input/text > output/texts/{{ loop.index0 }}
      iline=$tline
    {% endfor %}
  """
}

# BitextSelect
# ------------
# Common interface for sentence selection. Picks out the 'nlines' most relevant sentences from the training corpus.
#
BitextSelect: {
  input:  { src, trg, devSrc }
  output: { src, trg }

  nlines: ALL
}

MonolingMooreLewis: {
  extends: $BitextSelect
  # output: { src, trg }
}

BitextTokenizer: {
  input:  { src, trg }
  output: { tokSrc, tokTrg }

  # from Experiment: path for used script
  moses: $MOSES
  ncpus: $N_CPUS

  # from Experiment
  sourceLang: $_.sourceLang
  targetLang: $_.targetLang

  template: """
    {{ brick.moses }}/scripts/tokenizer/tokenizer.perl -threads {{ brick.ncpus }} -l {{ brick.sourceLang }} < input/src > output/tokSrc
    {{ brick.moses }}/scripts/tokenizer/tokenizer.perl -threads {{ brick.ncpus }} -l {{ brick.targetLang }} < input/trg > output/tokTrg
  """
}

BitextCleaner: {
  input:  { src, trg }
  output: { cleanSrc, cleanTrg }

  # from Experiment: path for used script
  moses: $MOSES

  # maximum number of words
  maxLen: 80

  template: """
    ln -sf src input/corpus.src
    ln -sf trg input/corpus.trg
    {{ brick.moses }}/scripts/training/clean-corpus-n.perl input/corpus src trg output/cleaned 1 {{ brick.maxLen }}
    ln -sf cleaned.src output/cleanSrc
    ln -sf cleaned.trg output/cleanTrg
  """
}

BitextTokenizeClean: {
  input:  { src, trg }
  output: {
    cleanSrc: $parts.BitextCleaner0.output.cleanSrc
    cleanTrg: $parts.BitextCleaner0.output.cleanTrg
  }

  parts: {
    BitextTokenizer0: {
      extends: $BitextTokenizer
      input:  { src: $_._.input.src, trg: $_._.input.trg }
      # output: { tokSrc, tokTrg }
    }
    BitextCleaner0: {
      extends: $BitextCleaner
      input:  { src: $BitextTokenizer0.output.tokSrc, trg: $BitextTokenizer0.output.tokTrg }
      # output: { cleanSrc, cleanTrg }
    }
  }
}

# BitextPrepareSplit
# ==================
# Tokenizes, cleans and splits a bitext corpus into train, dev and test parts.
#
BitextPrepareSplit: {
  input:  { bitextSrc, bitextTrg }
  # output: { devSrc, devRef, testSrc, testRef, trainSrc, trainTrg }

  output: {
    devSrc: $parts.SplitLinesSrc0.output.texts[0]
    devRef: $parts.SplitLinesTrg0.output.texts[0]

    testSrc: $parts.SplitLinesSrc0.output.texts[1]
    testRef: $parts.SplitLinesTrg0.output.texts[1]

    trainSrc: $parts.SplitLinesSrc0.output.texts[2]
    trainTrg: $parts.SplitLinesTrg0.output.texts[2]
  }

  nlinesSplit: [ 2000, 2000, END ]

  # TODO: should we shuffle before the split?

  parts: {
    BitextTokenizeClean0: {
      extends: $BitextTokenizeClean
      input:  { src: $_._.input.bitextSrc, trg: $_._.input.bitextTrg }
      # output: { cleanSrc, cleanTrg }
    }

    SplitLinesSrc0: {
      extends: $SplitLines

      input: { text: $BitextTokenizeClean0.output.cleanSrc }
      # output: { texts: [ True | i: [0..$nlines.length-1] ] }

      nlines: $nlinesSplit
    }

    SplitLinesTrg0: {
      extends: $SplitLines

      input: { text: $BitextTokenizeClean0.output.cleanTrg }
      # output: { texts: [ True | i: [0..$nlines.length-1] ] }

      nlines: $nlinesSplit
    }
  }
}

# Part-of-speech tagging
POS: @<Corpus/POS.cfg>
