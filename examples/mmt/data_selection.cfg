Bricks: @<Bricks.cfg>

MMTBaseline: @"baseline.cfg"


Hybridize: {
  input: { corpusPos, vocabulary }
  output: { corpusHybrid }

  # hybrid language word rank threshold
  rankThreshold

  # from Experiment
  bricksDir: $BRICKS

  template: """
    {{ brick.bricksDir }}/scripts/hybridize.py -t {{ brick.rankThreshold }} \\
      input/vocabulary < input/corpusPos > output/corpusHybrid
  """
}

GetVocabulary: {
  input: { corpus }
  output: { vocabulary }

  # from Experiment
  bricksDir: $BRICKS

  template: """
    {{ brick.bricksDir }}/scripts/get_vocabulary.py < input/corpus > output/vocabulary
  """
}


SelectionLM: {
  input:  { corpusPos, vocabulary }
  # languageModel: binarized KenLM
  output: { languageModelHybrid: $parts.LM0.output.languageModel }

  # hybrid language word rank threshold
  rankThreshold

  parts: {
    Hybridize0: {
      extends: $Hybridize

      input: { corpusPos: $_._.input.corpusPos, vocabulary: $_._.input.vocabulary }
      # output: { corpusHybrid }

      # from SelectionLM
      rankThreshold: $_.rankThreshold
    }

    LM0: {
      extends: $Bricks.LM.LanguageModelEstimator
      input:  { corpus: $Hybridize.output.corpusHybrid }
      # output: { languageModel }

      ngramOrder: 4
      prune: "0 0 1"
      extraOptions: "--discount_fallback"
    }
  }
}


Experiment: {
  extends: $Setup

  DESCRIPTION: """
    Gather sub-corpus data and prepare for intrinsic eval of training data
    selection.

    Includes randomly splitting off a small part (C_in) and "hiding" the rest
    in a mixed-domain haystack, as inspired by [Cuong & Sima'an 2014].

  """ + $corporaDescription


  # we hence expect train corpus to be ...:
  # * tokenized
  # * truecased (using the most common spelling)
  #     -- really, just want consistent casing with dev and test
  # * cleaned (tokens/sentence limited)

  # language pair
  sourceLang: en
  targetLang: it

  corpora: {
    #train: "ep"
    dev: "ep"
    test: "ep"
  }

  # must be before input: section
  trainCorpora: ["ep", "ibm", "ms"]

  input: {
    trainSources: [$corpora_truecased_train[$i] + $sourceLang | i: [0..$trainCorpora.length-1] ]
    trainTargets: [$corpora_truecased_train[$i] + $targetLang | i: [0..$trainCorpora.length-1] ]

    devSrc: $corpus_truecased_dev + $sourceLang
    devRef: $corpus_truecased_dev + $targetLang

    testSrc: $corpus_truecased_test + $sourceLang
    testRef: $corpus_truecased_test + $targetLang
  }

  output: {
  }

  # small|real
  variant: $MMTBaseline.Experiment.variant

  corpus_data: $MMT_CORPUS_DIR + "/" + $variant

  corpora_truecased_train: [ $corpus_data + "/" + $trainCorpora[$i] + "/train.clean." | i: [0..$trainCorpora.length-1] ]

  corpus_truecased_dev: $corpus_data + "/" + $corpora.dev + "/set1.clean."
  corpus_truecased_test: $corpus_data + "/" + $corpora.test + "/set2.clean."

  # #lines used for C_in, the in-domain corpus used to select "hidden" training data in the retrieval task.
  nlines_C_in: 20000
  # NOTE: should this be const for all corpora? (nb. different sizes...)

  # index of corpus to hide (essentially a C_in index)
  hideIndex: 0

  parts: {
    # POS tag all trainCorpora source sides
    PosTaggers0: {
      input: {
        trainSources: $_._.input.trainSources
        trainTargets: $_._.input.trainTargets
      }
      output: { corporaPos: [ $parts[$i].output.corpusPos ] }
      i: [0..$input.trainSources.length-1]
      parts: [{
        extends: $Bricks.Corpus.POS.PosTagger
        input: { corpus: $_._.input.trainSources[$i] }
        language: $sourceLang
      }]
    }

    # we run one selection experiment per corpus, each with their own C_in.

    ProcessCorpora0: {
      input: {
        trainSources: $_._.input.trainSources
        trainTargets: $_._.input.trainTargets
      }
      output: {
        corporaCinSrcPos: [ $parts[$i].output.cinSrcPos ]
        corporaChideSrcPos: [ $parts[$i].output.chideSrcPos ]
        corporaCinAlignment: [ $parts[$i].output.cinAlignment ]
        corporaChideAlignment: [ $parts[$i].output.chideAlignment ]

        corporaSrcPos: [ $parts[$i].output.srcPos ]
      }
      i: [0..$input.trainSources.length-1]
      parts: [{
        input: {
          src: $_._.input.trainSources[$i]
          trg: $_._.input.trainTargets[$i]
        }
        output: {
          cinSrcPos: $parts.SplitSrcLines0.output.texts[0]
          chideSrcPos: $parts.SplitSrcLines0.output.texts[1]

          cinAlignment: $parts.SplitAlignmentLines0.output.texts[0]
          chideAlignment: $parts.SplitAlignmentLines0.output.texts[1]

          srcPos: $parts.PosTaggerSrc0.output.corpusPos
        }

        # deterministic random seed (each Shuffle produces same ordering)
        corpusRandomSeed: 42

        parts: {
          WordAligner0: {
            extends: $Bricks.Giza.WordAligner
            input: {
              src: $_._.input.src
              trg: $_._.input.trg
            }
            # output: { alignment }
            cacheDir: $WORD_ALIGNMENT_CACHE_DIR
          }

          PosTaggerSrc0: {
            extends: $Bricks.Corpus.POS.PosTagger
            input: { corpus: $_._.input.src }
            # output: { corpusPos }
            language: $sourceLang
          }

          ShuffleSrc0: {
            extends: $Bricks.Corpus.Shuffle
            input:  { textFile: $PosTaggerSrc0.output.corpusPos }
            # output: { head }

            seed: $corpusRandomSeed
          }
          ShuffleAlignment0: {
            extends: $Bricks.Corpus.Shuffle
            input:  { textFile: $WordAligner0.output.alignment }
            # output: { head }

            seed: $corpusRandomSeed
          }

          # can't use?? why? output wiring complexity.
          #
          #ShuffleCorpora0: {
          #  extends: $Bricks.Corpus.ShuffleCorpora
          #  input: { textFiles: [
          #    $PosTaggerSrc0.output.corpusPos
          #    $WordAligner0.output.alignment
          #  ] }
          #  # output: { shuffledFiles: [] }
          #}

          # split into C_in and C_hide (hidden part)
          SplitSrcLines0: {
            extends: $Bricks.Corpus.SplitLines
            #input: { text: $ShuffleCorpora0.output.shuffledFiles[0] }
            input: { text: $ShuffleSrc0.output.head }
            # output: { texts: [] }
            nlines: [ $nlines_C_in, END ]
          }
          SplitAlignmentLines0: {
            extends: $Bricks.Corpus.SplitLines
            #input: { text: $ShuffleCorpora0.output.shuffledFiles[1] }
            input: { text: $ShuffleAlignment0.output.head }
            # output: { texts: [] }
            nlines: [ $nlines_C_in, END ]
          }
        }
      }]
    }

    # concatenate for vocabulary
    ConcatSrc0: {
      extends: $Bricks.Corpus.Concat
      input: { texts: $_._.input.trainSources }
      # output: { concat }
    }

    # global vocabulary to ensure identical word ranks
    GetVocabulary0: {
      extends: $GetVocabulary
      input: { corpus: $ConcatSrc0.output.concat }
      # output: { vocabulary }
    }

    # concatenate all corpora for general-domain selection LM
    ConcatSrcPos0: {
      extends: $Bricks.Corpus.Concat
      input: { texts: $ProcessCorpora0.output.corporaSrcPos }
      # output: { concat }
    }

    # general-domain selection LM
    GeneralLM0: {
      extends: $SelectionLM
      input:  { corpusPos: $ConcatSrcPos0.output.concat, vocabulary: $GetVocabulary0.output.vocabulary }
      # output: { languageModelHybrid }
    }


    # concatenate C_hide into C_mix
    # NOTE: how much do we hide? (proportions of hidden lines to C_mix)
    # currently, all of any given C_hide -> common C_mix for all
    # BUT we may want this more general.
    #CreateCmix0: {
    #  input: {  }
    #  output: { cmix }
    #}

    # run the selection experiment for each corpus.
    # i.e. every iteration, C_in is a different corpus, different C_mix, ...
    #
    ForeachCorpus0: {
      input: {
        fileList: [
          $ProcessCorpora0.output.corporaChideSrcPos
          $_._.input.trainTargets
          $ProcessCorpora0.output.corporaChideAlignment
        ]
        generalLM: $GeneralLM0.output.languageModelHybrid
        corporaCinSrcPos: $ProcessCorpora0.output.corporaCinSrcPos
        vocabulary: $GetVocabulary0.output.vocabulary
      }
      output: {}

      # outer $i
      i: [0..$trainCorpora.length-1]

      parts: [{
        input: {
          fileList: $_._.input.fileList
          generalLM: $_._.input.generalLM
          corpusCinSrcPos: $_._.input.corporaCinSrcPos[$i]
          vocabulary: $_._.input.vocabulary
        }
        output: {}

        # outer $i
        hideIndex: $i

        parts: {
          # create C_mix by hiding a specific amount of C_hide in a mixture of
          # other corpora.
          MixSelectionList0: {
            #input: {
            #  trainTargets: $_._.input.trainTargets
            #  corporaChideSrcPos: $ProcessCorpora0.output.corporaChideSrcPos
            #  corporaChideAlignment: $ProcessCorpora0.output.corporaChideAlignment
            #}
            input: {
              fileList: $_._.input.fileList
            }
            output: {
              #files: [ cmixSrc, cmixTrg, cmixAlignment ]
              #files: [ True, True, True ]
              files: [ $parts[$i].output.concat ]
            }

            # inner $i
            i: [0..$input.fileList.length-1]

            parts: [{
              input: {
                textFiles: $_._.input.fileList[$i]
              }
              output: {
                concat: $parts.ConcatAddChide0.output.concat
              }

              parts: {
                # concat everything except for C_hide
                ConcatExcept0: {
                  extends: $Bricks.Corpus.ConcatExcept
                  input:  { texts: $_._.input.textFiles }
                  # output: { concat }

                  exceptIndex: $hideIndex
                }

                # add specific amount of C_hide
                # note: C_hide is always at the beginning!
                # (currently, all of it... can introduce an amount later)
                ConcatAddChide0: {
                  extends: $Bricks.Corpus.Concat
                  input: { texts: [ $_._.input.textFiles[$hideIndex], $ConcatExcept0.output.concat ] }
                  # output: { concat }
                }
              }
            }]
          }

          # just provide sane naming again
          MixSelection0: {
            input: { selectionFiles: $MixSelectionList0.output.files }
            output: { cmixSrc, cmixTrg, cmixAlignment }
            template: """
              ln -sf ../input/selectionFiles/0 output/cmixSrc
              ln -sf ../input/selectionFiles/1 output/cmixTrg
              ln -sf ../input/selectionFiles/2 output/cmixAlignment
            """
          }

          # Build LM from C_in
          CinLM0: {
            extends: $SelectionLM
            input:  { corpusPos: $_._.input.corpusCinSrcPos, vocabulary: $_._.input.vocabulary }
            # output: { languageModelHybrid }
          }

          HybridMooreLewis0: {
            input: {
              generalLM: $_._.input.generalLM
              cinLM: $CinLM0.output.languageModelHybrid
              mixtureCorpus: $MixSelection0.output.cmixSrc
            }
            output: {
              # 1-based line permutation index file
              selection: $parts.MooreLewisRanking.output.selection
            }

            parts: {
              ScoreGeneral0: {
                extends: $Bricks.LM.ScoreLines
                input:  { text: $_._.input.mixtureCorpus, languageModel: $_._.input.generalLM }
                # output: { score }
              }
              ScoreCin0: {
                extends: $Bricks.LM.ScoreLines
                input:  { text: $_._.input.mixtureCorpus, languageModel: $_._.input.cinLM }
                # output: { score }
              }
              MooreLewisRanking: {
                input: { scoreGeneral: $ScoreGeneral0.output.score, scoreCin: $ScoreCin0.output.score }
                output: { selection }
                template: """
                  # compute cross-entropy difference H_in(s) - H_gen(s), add line number
                  paste input/scoreCin input/scoreGeneral | awk '{print ($1 - $2) "\t" NR}' > output/xe-diff
                  # float-numeric, descending sort
                	sort -g -r output/xe-diff > output/xe-diff.ranked
                	awk '{print $2}' output/xe-diff.ranked > output/xe-rank
                	ln -sf xe-rank output/selection
                	# clean up
                	rm -rf output/xe-diff.ranked output/xe-diff
                """
              }
            }
          }

          # general data-selection interface:
          # <nlines-select>
          # creates a 1-based line permutation index file with the first
          # nlines-select lines being
          # the selected data, and the rest below (intuitive: like sort output).
          #
          # easy to evaluate, since the first $nlines_C_in lines of C_mix
          # are always C_in.
        }
      }]
    }
  }
}
