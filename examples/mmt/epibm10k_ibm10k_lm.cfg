Bricks: @<Bricks.cfg>

# overridden in $Setup / Hopper.cfg only, to simplify testing experiments
MMT_VARIANT: $Setup.MMT_VARIANTS.real

EPIBM10k_IBM10k: @"epibm10k_ibm10k.cfg"

Experiment: {
  extends: $EPIBM10k_IBM10k.Experiment

  DESCRIPTION: """
    Phrase-based model with lexicalized reordering.
    3-domain MMT data. Concatenated training corpus, see below.
    Domain adaptation scenario: interpolation of two TMs and two LMs.

    Tests the interpolation between ep_ibm10k concatenation and another ibm10k portion.
    So 20k of IBM went in here, but 10k of those was hidden along with a larger EP corpus.

    word alignment: jointly word-aligned full-length training corpora.
    language models: from the first training part, and 20k data (to control LM for checking TM adaptation vs. ep_ibm20k)
    translation models: from the two training parts

  """ + $corporaDescription

  parts: {
    # inherit all parts, except for the override below.
    extends: $EPIBM10k_IBM10k.Experiment.parts

    LMTrain1: {
      extends: $Bricks.Corpus.Head
      input:  { textFile: $_._.input.trainTargets[1] }
      # output: { head }

      # override to get leading nlines lines
      nlines: 20000
    }

    LanguageModel1: {
      extends: $Bricks.LM.LanguageModelEstimator

      # full-length without limit (all of IBM)
      # $_._.input.trainTargets[1]

      # full-length train targets without limit (second ibm10k)
      # input:  { corpus: $TrainingCorpus0.output.trainTargets[2] }

      # limited length train targets (second ibm10k)
      #input:  { corpus: $LimitTrainCorpora0.output.targets[2] }

      # 20k of IBM
      input:  { corpus: $LMTrain1.output.head }

      # languageModel: binarized KenLM
      # output: { languageModel }
    }
  }
}
