Bricks: @<Bricks.cfg>

EPIBM10k_IBM10k: @"epibm10k_ibm10k.cfg"

Experiment: {
  extends: $EPIBM10k_IBM10k.Experiment

  corpora: {
    train: "(ep)+(ibm10k)"  # now only used for corporaDescription
    dev: "ibm"
    test: "ibm"
  }

  # must be before input: section
  trainCorpora: ["ep", "ibm"]

  input: {
    trainSources: [$corpora_truecased_train[$i] + $sourceLang | i: [0..$trainCorpora.length-1] ]
    trainTargets: [$corpora_truecased_train[$i] + $targetLang | i: [0..$trainCorpora.length-1] ]

    devSrc: $corpus_truecased_dev + $sourceLang
    devRef: $corpus_truecased_dev + $targetLang

    testSrc: $corpus_truecased_test + $sourceLang
    testRef: $corpus_truecased_test + $targetLang
  }

  nlinesTrain: [ALL, 10000]

  parts: {
    # inherit all parts, except for the overrides below.
    extends: $EPIBM10k_IBM10k.Experiment.parts

    # overrides $EPIBM10k_IBM10k.Experiment.PhraseTable0
    PhraseTable0: {
      extends: $Bricks.Phrase.DevTestFilteredPhraseTable

      input:  {
        # use ep corpus
        src: $LimitTrainCorpora0.output.sources[0]
        trg: $LimitTrainCorpora0.output.targets[0]
        alignment: $JointWordAligner0.output.alignments[0]

        devSrc: $_._.input.devSrc
        testSrc: $_._.input.testSrc
      }
    }

    # note: would be nice if we had $PhraseTable taking lists and concatenating itself.
    # (then the two PhraseTables would look very similar here)

    # overrides $EPIBM10k_IBM10k.Experiment.PhraseTable1
    PhraseTable1: {
      extends: $Bricks.Phrase.DevTestFilteredPhraseTable

      input:  {
        # use ibm corpus
        src: $LimitTrainCorpora0.output.sources[1]
        trg: $LimitTrainCorpora0.output.targets[1]
        alignment: $JointWordAligner0.output.alignments[1]

        devSrc: $_._.input.devSrc
        testSrc: $_._.input.testSrc
      }
    }


    # training corpus with modified splitting
    #
    # overrides $EPIBM10k_IBM10k.Experiment.TrainingCorpus0
    TrainingCorpus0: {
      input:  {
        #               ep                          first ibm10k (really, all of IBM for aligning)
        trainSources: [ $_._.input.trainSources[0], $_._.input.trainSources[1] ]
        trainTargets: [ $_._.input.trainTargets[0], $_._.input.trainTargets[1] ]
      }
      output: {
        trainSources: [ True, True ]  # or [ True | i: [0..$input.trainSources.length] ]
        trainTargets: [ True, True ]
      }
      template: """
        mkdir -p output/trainSources output/trainTargets

        {% for source in brick.input.trainSources %}
          ln -sf ../../input/trainSources/{{ loop.index0 }} output/trainSources/{{ loop.index0 }}
          ln -sf ../../input/trainTargets/{{ loop.index0 }} output/trainTargets/{{ loop.index0 }}
        {% endfor %}
      """
    }

    # overrides $EPIBM10k_IBM10k.Experiment.LimitTrainCorpora0
    # (necessary for nlinesTrain.length -- workaround)
    LimitTrainCorpora0: {
      input:  {
        sources: $TrainingCorpus0.output.trainSources
        targets: $TrainingCorpus0.output.trainTargets
      }
      output: {
        sources: [$parts[$i].output.src]
        targets: [$parts[$i].output.trg]
      }

      i: [0..$nlinesTrain.length-1]

      parts: [{
        extends: $Bricks.Corpus.BitextHead

        input:  {
          src: $_._.input.sources[$i]
          trg: $_._.input.targets[$i]
        }
        # output: { src, trg }

        nlines: $nlinesTrain[$i]
      }]
    }

    # overrides $EPIBM10k_IBM10k.Experiment.LimitTrainCorpora0
    LMTrain0: {
      input:  { texts: [ $LimitTrainCorpora0.output.targets[0] ] }
      output: { concat }

      # empty brick
    }

    # overrides $EPIBM10k_IBM10k.Experiment.LanguageModel0
    LanguageModel0: {
      extends: $Bricks.LM.LanguageModelEstimator

      input:  { corpus: $LimitTrainCorpora0.output.targets[0] }
      # output: { languageModel }
    }

    # overrides $EPIBM10k_IBM10k.Experiment.LanguageModel1
    LanguageModel1: {
      extends: $Bricks.LM.LanguageModelEstimator

      input:  { corpus: $LimitTrainCorpora0.output.targets[1] }
      # output: { languageModel }
    }

    # overrides $EPIBM10k_IBM10k.Experiment.JointWordAligner0
    # (necessary for trainCorpora.length -- workaround)
    JointWordAligner0: {
      extends: $Bricks.Giza.LimitJointWordAligner

      input:  {
        sources: $TrainingCorpus0.output.trainSources
        targets: $TrainingCorpus0.output.trainTargets
      }

      # we have to define output list with the correct length.
      # (this cannot currently go into our parent, because list comprehensions are not lazy, but are evaluated at parse time.)
      # output: { alignments: $parts.Split0.output.texts }
      output: {
        alignments: [ $parts.Split0.output.texts[$i] | i: [0..$trainCorpora.length-1] ]
      }

      # from Experiment
      nlinesLimit: $_._.nlinesTrain
    }

    # overrides $EPIBM10k_IBM10k.Experiment.WordAligner0
    WordAligner0: {
      input:  { alignments: [ $JointWordAligner0.output.alignments[$i] | i: [0..1] ] }  # first two corpus parts
      output: { alignment }

      # empty brick
    }
  }
}
