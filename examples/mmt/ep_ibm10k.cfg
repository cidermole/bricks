Bricks: @<Bricks.cfg>

MMTBaseline: @"baseline.cfg"

Experiment: {
  extends: $MMTBaseline.Experiment

  DESCRIPTION: """
    Phrase-based model with lexicalized reordering.
    3-domain MMT data. Concatenated training corpus, see below.

    Domain adaptation scenario using mixed training data.

    word alignment: jointly word-aligned full-length training corpora.
    language model: from target side of full-length train corpus
    translation model: from length-limited train corpus

  """ + $corporaDescription

  # must be before input: section
  trainCorpora: ["ep", "ibm"]

  input: {
    trainSources: [$corpora_truecased_train[$i] + $sourceLang | i: [0..$trainCorpora.length-1] ]
    trainTargets: [$corpora_truecased_train[$i] + $targetLang | i: [0..$trainCorpora.length-1] ]

    devSrc: $corpus_truecased_dev + $sourceLang
    devRef: $corpus_truecased_dev + $targetLang

    testSrc: $corpus_truecased_test + $sourceLang
    testRef: $corpus_truecased_test + $targetLang
  }

  output: {
    hyp: $parts.TestMoses0.output.hyp
    score: $parts.Bleu0.output.score
    summary: $parts.Summary0.output.summary

    alignments: $parts.JointWordAligner0.output.alignments
  }

  corpora: {
    train: "ep+ibm10k"  # now only used for corporaDescription
    dev: "ibm"
    test: "ibm"
  }

  corpora_truecased_train: [ $corpus_data + "/" + $trainCorpora[$i] + "/train.clean." | i: [0..$trainCorpora.length-1] ]

  # we now make this a list!
  # must have same length as trainSources, trainTargets
  nlinesTrain: [ALL, 10000]

  parts: {
    # inherit all parts, except for the override below.
    extends: $MMTBaseline.Experiment.parts

    # overrides $MMTBaseline.Experiment.parts.LimitTrain0
    LimitTrain0: {
      extends: $Bricks.Corpus.BitextConcat

      input:  {
        sources: $LimitTrainCorpora0.output.sources
        targets: $LimitTrainCorpora0.output.targets
      }
      # output: { src, trg }
    }

    LimitTrainCorpora0: {
      input:  {
        sources: $_._.input.trainSources
        targets: $_._.input.trainTargets
      }
      output: {
        sources: [$parts[$i].output.src]
        targets: [$parts[$i].output.trg]
      }

      i: [0..$nlinesTrain.length-1]

      parts: [{
        extends: $Bricks.Corpus.BitextHead

        input:  {
          src: $_._.input.sources[$i]
          trg: $_._.input.targets[$i]
        }
        # output: { src, trg }

        nlines: $nlinesTrain[$i]
      }]
    }

    FullTrgTrain0: {
      extends: $Bricks.Corpus.Concat

      input:  { texts: $_._.input.trainTargets }
      # output: { concat }
    }

    # overrides $MMTBaseline.Experiment.parts.LanguageModel0
    LanguageModel0: {
      extends: $Bricks.LM.LanguageModelEstimator

      # full-length
      # input:  { corpus: $FullTrgTrain0.output.concat }

      # limited train length
      input:  { corpus: $LimitTrain0.output.trg }

      # languageModel: binarized KenLM
      # output: { languageModel }
    }

    JointWordAligner0: {
      extends: $Bricks.Giza.LimitJointWordAligner

      input:  {
        sources: $_._.input.trainSources
        targets: $_._.input.trainTargets
      }

      # we have to define output list with the correct length.
      # (this cannot currently go into our parent, because list comprehensions are not lazy, but are evaluated at parse time.)
      # output: { alignments: $parts.Split0.output.texts }
      output: {
        alignments: [ $parts.Split0.output.texts[$i] | i: [0..$trainCorpora.length-1] ]
      }

      # from Experiment
      nlinesLimit: $_._.nlinesTrain
    }

    # paste together the limited word alignments.
    #
    # overrides $MMTBaseline.Experiment.parts.WordAligner0
    WordAligner0: {
      input:  { alignments: $JointWordAligner0.output.alignments }
      output: { alignment: $parts.Concat0.output.concat }

      parts: {
        Concat0: {
          extends: $Bricks.Corpus.Concat
          input:   { texts: $_._.input.alignments }
          # output: { concat }
        }
      }
    }
  }
}
