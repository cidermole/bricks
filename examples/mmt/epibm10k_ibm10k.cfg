Bricks: @<Bricks.cfg>

MMTBaseline: @"baseline.cfg"

Experiment: {
  extends: $MMTBaseline.Experiment

  DESCRIPTION: """
    Phrase-based model with lexicalized reordering.
    3-domain MMT data. Concatenated training corpus, see below.
    Domain adaptation scenario using mixed training data.

    Tests the interpolation between ep_ibm10k concatenation and another ibm10k portion.
    So 20k of IBM went in here, but 10k of those was hidden along with a larger EP corpus.

    word alignment: jointly word-aligned full-length training corpora.
    language model: from target side of full-length train corpus
    translation model: from length-limited train corpus

  """ + $corporaDescription


  # NOTE: training corpus must be in randomized sentence order
  # for nlinesTrain to work properly.

  # we hence expect train corpus to be ...:
  # * tokenized
  # * truecased (using the most common spelling)
  #     -- really, just want consistent casing with dev and test
  # * cleaned (tokens/sentence limited)

  # language pair
  sourceLang: en
  targetLang: it


  # must be before input: section
  trainCorpora: ["ep", "ibm", "ibm"]

  input: {
    trainSources: [$corpora_truecased_train[$i] + $sourceLang | i: [0..$trainCorpora.length-1] ]
    trainTargets: [$corpora_truecased_train[$i] + $targetLang | i: [0..$trainCorpora.length-1] ]

    devSrc: $corpus_truecased_dev + $sourceLang
    devRef: $corpus_truecased_dev + $targetLang

    testSrc: $corpus_truecased_test + $sourceLang
    testRef: $corpus_truecased_test + $targetLang
  }

  output: {
    hyp: $parts.TestMoses0.output.hyp
    score: $parts.Bleu0.output.score
    summary: $parts.Summary0.output.summary

    alignments: $parts.JointWordAligner0.output.alignments
  }

  corpora: {
    train: "(ep+ibm10k)+(ibm10k)"  # now only used for corporaDescription
    dev: "ibm"
    test: "ibm"
  }

  corpus_data: $MMT_CORPUS_DIR + "/" + $variant

  corpora_truecased_train: [ $corpus_data + "/" + $trainCorpora[$i] + "/train.clean." | i: [0..$trainCorpora.length-1] ]

  corpus_truecased_dev: $corpus_data + "/" + $corpora.dev + "/set1.clean."
  corpus_truecased_test: $corpus_data + "/" + $corpora.test + "/set2.clean."

  corporaDescription: "train: " + $corpora.train + " dev: " + $corpora.dev + " test: " + $corpora.test + " "


  # we now make this a list!
  # must have same length as trainSources, trainTargets
  nlinesTrain: [ALL, 10000, 10000]

  # split of ibm corpus into multiple parts
  nlinesSplitCorpus1: [10000, END]

  parts: {
    # inherit all parts, except for the override below.
    #extends: $MMTBaseline.Experiment.parts

    # ep_ibm10k
    PhraseTable0: {
      extends: $Bricks.Phrase.DevTestFilteredPhraseTable

      input:  {
        src: $LimitTrain0.output.src
        trg: $LimitTrain0.output.trg
        alignment: $WordAligner0.output.alignment

        devSrc: $_._.input.devSrc
        testSrc: $_._.input.testSrc
      }
    }

    # note: would be nice if we had $PhraseTable taking lists and concatenating itself.
    # (then the two PhraseTables would look very similar here)

    # another, different ibm10k
    PhraseTable1: {
      extends: $Bricks.Phrase.DevTestFilteredPhraseTable

      input:  {
        # use third corpus
        src: $LimitTrainCorpora0.output.sources[2]
        trg: $LimitTrainCorpora0.output.targets[2]
        alignment: $JointWordAligner0.output.alignments[2]

        devSrc: $_._.input.devSrc
        testSrc: $_._.input.testSrc
      }
    }

    TestMosesIni0: {
      extends: $Bricks.Moses.MosesIni
      input: {
        languageModels: [
          $LanguageModel0.output.languageModel
          $LanguageModel1.output.languageModel
        ]
        phraseTables: []
        reorderingTables: []
        binaryPhraseTables: [
          $PhraseTable0.output.testPhraseTableDir
          $PhraseTable1.output.testPhraseTableDir
        ]
        binaryReorderingTables: [
          $PhraseTable0.output.testReorderingTableDir
          $PhraseTable1.output.testReorderingTableDir
        ]
      }
      # output: { mosesIni }
    }

    WeightedTestMosesIni0: {
      extends: $Bricks.Moses.WeightedMosesIni
      input: {
        generatedMosesIni: $TestMosesIni0.output.mosesIni
        tunedMosesIni: $Mert0.output.tunedMosesIni
      }
      # output: { weightedMosesIni }
    }

    DevMosesIni0: {
      extends: $Bricks.Moses.MosesIni
      input: {
        languageModels: [
          $LanguageModel0.output.languageModel
          $LanguageModel1.output.languageModel
        ]
        phraseTables: []
        reorderingTables: []
        binaryPhraseTables: [
          $PhraseTable0.output.devPhraseTableDir
          $PhraseTable1.output.devPhraseTableDir
        ]
        binaryReorderingTables: [
          $PhraseTable0.output.devReorderingTableDir
          $PhraseTable1.output.devReorderingTableDir
        ]
      }
      # output: { mosesIni }
    }

    Mert0: {
      extends: $Bricks.Moses.Mert

      input: {
        mosesIni: $DevMosesIni0.output.mosesIni
        devSrc: $_._.input.devSrc
        devRef: $_._.input.devRef
      }
      # output: { tunedMosesIni }
    }

    TestMoses0: {
      extends: $Bricks.Moses.Decoder
      input: {
        mosesIni: $WeightedTestMosesIni0.output.weightedMosesIni
        src: $_._.input.testSrc
      }
      # output: { hyp }
    }

    Bleu0: {
      extends: $Bricks.Eval.Bleu
      input: { hyp: $TestMoses0.output.hyp, ref: $_._.input.testRef }
      # output: { score }
    }

    # write a human-readable summary of the experiment and its results.
    Summary0: {
      extends: $Bricks.Summary
      input:  { results: [$Bleu0.output.score] }
      # output: { summary }
    }





    # split up corpus 1
    SplitLinesSrc1: {
      extends: $Bricks.Corpus.SplitLines

      # last entry may contain END
      nlines: $nlinesSplitCorpus1

      input: { text: $_._.input.trainSources[1] }
      output: { texts: [ True, True ] }
    }
    SplitLinesTrg1: {
      extends: $Bricks.Corpus.SplitLines

      # last entry may contain END
      nlines: $nlinesSplitCorpus1

      input: { text: $_._.input.trainTargets[1] }
      output: { texts: [ True, True ] }
    }


    # training corpus with modified splitting
    TrainingCorpus0: {
      input:  {
        #               ep                          first ibm10k                    second ibm10k
        trainSources: [ $_._.input.trainSources[0], $SplitLinesSrc1.output.texts[0], $SplitLinesSrc1.output.texts[1] ]
        trainTargets: [ $_._.input.trainTargets[0], $SplitLinesTrg1.output.texts[0], $SplitLinesTrg1.output.texts[1] ]
      }
      output: {
        trainSources: [ True, True, True ]  # or [ True | i: [0..$input.trainSources.length] ]
        trainTargets: [ True, True, True ]
      }
      template: """
        mkdir -p output/trainSources output/trainTargets

        {% for source in brick.input.trainSources %}
          ln -sf ../../input/trainSources/{{ loop.index0 }} output/trainSources/{{ loop.index0 }}
          ln -sf ../../input/trainTargets/{{ loop.index0 }} output/trainTargets/{{ loop.index0 }}
        {% endfor %}
      """
    }



    # TODO fix name (no limit here, just concat)
    LimitTrain0: {
      extends: $Bricks.Corpus.BitextConcat

      input:  {
        sources: [ $TrainingCorpus0.output.trainSources[$i] | i: [0..1] ]  # first two corpus parts
        targets: [ $TrainingCorpus0.output.trainTargets[$i] | i: [0..1] ]  # first two corpus parts
      }
      # output: { src, trg }
    }

    LimitTrainCorpora0: {
      input:  {
        sources: $TrainingCorpus0.output.trainSources
        targets: $TrainingCorpus0.output.trainTargets
      }
      output: {
        sources: [$parts[$i].output.src]
        targets: [$parts[$i].output.trg]
      }

      i: [0..$nlinesTrain.length-1]

      parts: [{
        extends: $Bricks.Corpus.BitextHead

        input:  {
          src: $_._.input.sources[$i]
          trg: $_._.input.targets[$i]
        }
        # output: { src, trg }

        nlines: $nlinesTrain[$i]
      }]
    }

    LMTrain0: {
      extends: $Bricks.Corpus.Concat

      # full-length train targets without limit (first 2 parts)
      # input:  { texts: [ $TrainingCorpus0.output.trainTargets[$i] | i: [0..1] ] }

      # limited length train targets (first 2 parts)
      input:  { texts: [ $LimitTrainCorpora0.output.targets[$i] | i: [0..1] ] }

      # output: { concat }
    }

    LanguageModel0: {
      extends: $Bricks.LM.LanguageModelEstimator

      input:  { corpus: $LMTrain0.output.concat }
      # languageModel: binarized KenLM
      # output: { languageModel }
    }

    # it would be nice if we had an LM training from a (concatenation of a) list of corpora.
    # (combine two bricks above and extract into Bricks.cfg)

    LanguageModel1: {
      extends: $Bricks.LM.LanguageModelEstimator

      # full-length train targets without limit (second ibm10k)
      # input:  { corpus: $TrainingCorpus0.output.trainTargets[2] }

      # limited length train targets (second ibm10k)
      input:  { corpus: $LimitTrainCorpora0.output.targets[2] }

      # languageModel: binarized KenLM
      # output: { languageModel }
    }

    JointWordAligner0: {
      extends: $Bricks.Giza.LimitJointWordAligner

      input:  {
        sources: $TrainingCorpus0.output.trainSources
        targets: $TrainingCorpus0.output.trainTargets
      }

      # we have to define output list with the correct length.
      # (this cannot currently go into our parent, because list comprehensions are not lazy, but are evaluated at parse time.)
      # output: { alignments: $parts.Split0.output.texts }
      output: {
        alignments: [ $parts.Split0.output.texts[$i] | i: [0..$trainCorpora.length-1] ]
      }

      # from Experiment
      nlinesLimit: $_._.nlinesTrain
    }

    # paste together the limited word alignments.
    #
    WordAligner0: {
      input:  { alignments: [ $JointWordAligner0.output.alignments[$i] | i: [0..1] ] }  # first two corpus parts
      output: { alignment: $parts.Concat0.output.concat }

      parts: {
        Concat0: {
          extends: $Bricks.Corpus.Concat
          input:   { texts: $_._.input.alignments }
          # output: { concat }
        }
      }
    }
  }
}
